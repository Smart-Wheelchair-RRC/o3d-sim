{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "import open3d as o3d\n",
    "from cuml.cluster import DBSCAN\n",
    "import cupy as cp\n",
    "\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "scripts_dir = os.path.abspath(os.path.join(current_dir, '..', 'scripts'))\n",
    "sys.path.append(scripts_dir)\n",
    "\n",
    "from iou import compute_3d_iou_accuracte_batch, compute_3d_iou, compute_iou_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pastel_color():\n",
    "    # generate (r, g, b) tuple of random numbers between 0.5 and 1, truncate to 2 decimal places\n",
    "    r = round(random.uniform(0.5, 1), 2)\n",
    "    g = round(random.uniform(0.5, 1), 2)\n",
    "    b = round(random.uniform(0.5, 1), 2)\n",
    "    return (r, g, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "img_dict = {img_name: {img_path: str,\n",
    "                        ram_tags: list_of_str,\n",
    "                        objs: {0: {bbox: [x1, y1, x2, y2],\n",
    "                                    phrase: str,\n",
    "                                    clip_embed: [1, 1024]},\n",
    "                                    dino_embed: [1, 1024]},\n",
    "                                    mask: [h, w],\n",
    "                                    prob: float,\n",
    "                                    aabb: arr}\n",
    "                                1: {...},\n",
    "                        }\n",
    "            img_name: {...},\n",
    "            }\n",
    "'''\n",
    "\n",
    "def get_depth(img_name, depth_scale):\n",
    "    # depth_path = os.path.join(depth_dir, img_name + '.npy')\n",
    "    # depth = np.load(depth_path)\n",
    "\n",
    "    depth_path = os.path.join(depth_dir, img_name + '.png')\n",
    "    depth = cv2.imread(depth_path, cv2.IMREAD_ANYDEPTH)\n",
    "    depth = depth.astype(np.float32) / depth_scale\n",
    "    return depth\n",
    "\n",
    "def get_pose(img_name):\n",
    "    pose_path = os.path.join(pose_dir, img_name + '.txt')\n",
    "\n",
    "    # check if the pose file exists, if it doesn't, return None\n",
    "    # [x, y, z, qx, qy, qz, qw]\n",
    "    if not os.path.exists(pose_path):\n",
    "        return None\n",
    "    \n",
    "    with open(pose_path, 'r') as f:\n",
    "        pose = f.read().split()\n",
    "        pose = np.array(pose).astype(np.float32)\n",
    "\n",
    "        # #change pose from [x, y, z, qw, qx, qy, qz] to [x, y, z, qx, qy, qz, qw]\n",
    "        # pose = np.concatenate((pose[:3], pose[4:], pose[3:4]))\n",
    "    return pose\n",
    "\n",
    "def get_sim_cam_mat_with_fov(h, w, fov):\n",
    "    cam_mat = np.eye(3)\n",
    "    cam_mat[0, 0] = cam_mat[1, 1] = w / (2.0 * np.tan(np.deg2rad(fov / 2)))\n",
    "    cam_mat[0, 2] = w / 2.0\n",
    "    cam_mat[1, 2] = h / 2.0\n",
    "    return cam_mat\n",
    "\n",
    "def get_realsense_cam_mat():\n",
    "    K = np.array([[386.458, 0, 321.111],\n",
    "              [0, 386.458, 241.595],\n",
    "              [0, 0, 1]])\n",
    "    return K\n",
    "\n",
    "def get_kinect_cam_mat():\n",
    "    K = np.array([[9.7096624755859375e+02, 0., 1.0272059326171875e+03], \n",
    "                  [0., 9.7109600830078125e+02, 7.7529718017578125e+02], \n",
    "                  [0., 0., 1]]) #for wheel\n",
    "    \n",
    "    # K = np.array([[5.0449380493164062e+02, 0., 3.3090179443359375e+02], \n",
    "    #               [0., 5.0470922851562500e+02, 3.1253039550781250e+02], \n",
    "    #               [0., 0., 1.]]) # for handheld\n",
    "    return K\n",
    "\n",
    "def get_ipad_cam_mat():\n",
    "    K = np.array([[7.9555474853515625e+02, 0., 3.6264770507812500e+02], \n",
    "                  [0., 7.9555474853515625e+02, 4.7412319946289062e+02], \n",
    "                  [0., 0., 1.]])\n",
    "    \n",
    "    # Downsample factor\n",
    "    downsample_factor = 3.75\n",
    "\n",
    "    # Scale down the camera matrix\n",
    "    scaled_K = K.copy()\n",
    "    scaled_K[0, 0] /= downsample_factor  # fx\n",
    "    scaled_K[1, 1] /= downsample_factor  # fy\n",
    "    scaled_K[0, 2] /= downsample_factor  # cx\n",
    "    scaled_K[1, 2] /= downsample_factor  # cy\n",
    "    \n",
    "    return scaled_K\n",
    "\n",
    "def get_iphone_cam_mat():\n",
    "    K = np.array([[6.6585675048828125e+02, 0., 3.5704681396484375e+02], \n",
    "                 [0., 6.6585675048828125e+02, 4.8127374267578125e+02], \n",
    "                 [0., 0., 1. ]])\n",
    "    \n",
    "    # Downsample factor\n",
    "    downsample_factor = 3.75\n",
    "\n",
    "    # Scale down the camera matrix\n",
    "    scaled_K = K.copy()\n",
    "    scaled_K[0, 0] /= downsample_factor  # fx\n",
    "    scaled_K[1, 1] /= downsample_factor  # fy\n",
    "    scaled_K[0, 2] /= downsample_factor  # cx\n",
    "    scaled_K[1, 2] /= downsample_factor  # cy\n",
    "\n",
    "    return scaled_K\n",
    "\n",
    "def get_mp3d_cam_mat():\n",
    "    K = np.array([[ 400.,    0., 400.],\n",
    "                  [   0., 400., 400.],\n",
    "                  [   0.,    0.,    1.]])\n",
    "    return K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_point_cloud(img_id, obj_data, cam_mat, color=(1, 0, 0), cam_height=1.5, depth_scale=1000.0):\n",
    "    \"\"\"\n",
    "    Generates a point cloud from a depth image, camera intrinsics, mask, and pose.\n",
    "    Only points within the mask and with valid depth are added to the cloud.\n",
    "    Points are colored using the specified color.\n",
    "    \"\"\"\n",
    "    depth = get_depth(img_id, depth_scale=depth_scale)\n",
    "    pose = get_pose(img_id)\n",
    "    mask = obj_data['mask']\n",
    "\n",
    "    if pose is None:\n",
    "        return o3d.geometry.PointCloud()\n",
    "\n",
    "    # Reproject the depth to 3D space\n",
    "    rows, cols = np.where(mask)\n",
    "\n",
    "    \n",
    "    depth_values = depth[rows, cols]\n",
    "    valid_depth_indices = (depth_values > 0) & (depth_values <= 10)\n",
    "\n",
    "    rows = rows[valid_depth_indices]\n",
    "    cols = cols[valid_depth_indices]\n",
    "    depth_values = depth_values[valid_depth_indices]\n",
    "\n",
    "    points2d = np.vstack([cols, rows, np.ones_like(rows)])\n",
    "\n",
    "    cam_mat_inv = np.linalg.inv(cam_mat)\n",
    "    points3d_cam = cam_mat_inv @ points2d * depth_values\n",
    "    points3d_homo = np.vstack([points3d_cam, np.ones((1, points3d_cam.shape[1]))])\n",
    "\n",
    "    # Parse the pose\n",
    "    pos = np.array(pose[:3], dtype=float).reshape((3, 1))\n",
    "    quat = pose[3:]\n",
    "    rot = R.from_quat(quat).as_matrix()\n",
    "\n",
    "    # # Apply rotation correction, to match the orientation z: backward, y: upward, and x: right\n",
    "    # rot_ro_cam = np.eye(3)\n",
    "    # rot_ro_cam[1, 1] = -1\n",
    "    # rot_ro_cam[2, 2] = -1\n",
    "\n",
    "    # rot = rot @ rot_ro_cam\n",
    "\n",
    "    # # Apply position correction\n",
    "    # pos[1] += cam_height\n",
    "\n",
    "    # Create the pose matrix\n",
    "    pose_matrix = np.eye(4)\n",
    "    pose_matrix[:3, :3] = rot\n",
    "    pose_matrix[:3, 3] = pos.reshape(-1)\n",
    "\n",
    "    points3d_global_homo = pose_matrix @ points3d_homo\n",
    "    points3d_global = points3d_global_homo[:3, :]\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points3d_global.T)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(np.tile(color, (points3d_global.shape[1], 1)))\n",
    "\n",
    "    # # Additional rotation to get the points in the correct orientation\n",
    "    # new_rot = np.array([[1.0, 0, 0], [0, 0, -1.0], [0, 1.0, 0]])\n",
    "    # new_rot_matrix = np.eye(4)\n",
    "    # new_rot_matrix[:3, :3] = new_rot\n",
    "\n",
    "    # # Apply new_rot to the point cloud using the transform function\n",
    "    # pcd.transform(new_rot_matrix)\n",
    "\n",
    "    return pcd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_DBSCAN(point_cloud_o3d, eps=0.2, min_samples=20):\n",
    "\n",
    "    if point_cloud_o3d.is_empty():\n",
    "        return point_cloud_o3d\n",
    "\n",
    "    # Convert Open3D point cloud to NumPy arrays\n",
    "    points_np = np.asarray(point_cloud_o3d.points)\n",
    "    colors_np = np.asarray(point_cloud_o3d.colors)\n",
    "\n",
    "    # depending on number of points, choose int32 or int64\n",
    "    \n",
    "\n",
    "    # Convert NumPy array to CuPy array for GPU computations\n",
    "    points_gpu = cp.asarray(points_np)\n",
    "\n",
    "    # Create a DBSCAN instance with cuML\n",
    "    dbscan_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "\n",
    "    # Fit the model to the GPU data\n",
    "    dbscan_model.fit(points_gpu)\n",
    "\n",
    "    # Get the labels for the clusters\n",
    "    labels_gpu = dbscan_model.labels_\n",
    "\n",
    "    # Convert the labels back to a NumPy array\n",
    "    labels = cp.asnumpy(labels_gpu)\n",
    "\n",
    "    # Count the occurrence of each label to find the largest cluster\n",
    "    label_counter = Counter(labels)\n",
    "    label_counter.pop(-1, None)  # Remove the noise label (-1)\n",
    "    if not label_counter:  # If all points are noise, return an empty point cloud\n",
    "        return o3d.geometry.PointCloud()\n",
    "\n",
    "    # Find the label of the largest cluster\n",
    "    largest_cluster_label = max(label_counter, key=label_counter.get)\n",
    "\n",
    "    # Filter the points and colors that belong to the largest cluster\n",
    "    largest_cluster_points = points_np[labels == largest_cluster_label]\n",
    "    largest_cluster_colors = colors_np[labels == largest_cluster_label]\n",
    "\n",
    "    # Create a new Open3D point cloud with the points and colors of the largest cluster\n",
    "    largest_cluster_point_cloud_o3d = o3d.geometry.PointCloud()\n",
    "    largest_cluster_point_cloud_o3d.points = o3d.utility.Vector3dVector(largest_cluster_points)\n",
    "    largest_cluster_point_cloud_o3d.colors = o3d.utility.Vector3dVector(largest_cluster_colors)\n",
    "\n",
    "    return largest_cluster_point_cloud_o3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cosine_similarity(vec1, vec2):\n",
    "    # Ensure the vectors have the same shape\n",
    "    if vec1.shape != vec2.shape:\n",
    "        raise ValueError(\"Vectors must have the same shape.\")\n",
    "\n",
    "    # Compute the dot product of the vectors\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "\n",
    "    # Compute the magnitudes (Euclidean norms) of the vectors\n",
    "    magnitude_vec1 = np.linalg.norm(vec1)\n",
    "    magnitude_vec2 = np.linalg.norm(vec2)\n",
    "\n",
    "    # Compute the cosine similarity\n",
    "    similarity = dot_product / (magnitude_vec1 * magnitude_vec2)\n",
    "\n",
    "    # Normalize the similarity value to [0, 1]\n",
    "    normalized_similarity = 0.5 * (similarity + 1)\n",
    "\n",
    "    return normalized_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert Open3D point cloud to NumPy array\n",
    "def pointcloud_to_numpy(pcd):\n",
    "    return np.asarray(pcd.points)\n",
    "    # return np.asarray(pcd.points), np.asarray(pcd.colors)\n",
    "\n",
    "# Function to convert NumPy array to Open3D point cloud\n",
    "def numpy_to_pointcloud(points):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    return pcd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_icp(source, target, params):\n",
    "    # Set ICP configuration\n",
    "    config = o3d.pipelines.registration.ICPConvergenceCriteria(relative_fitness=1e-6,\n",
    "                                                            relative_rmse=1e-6, max_iteration=params['icp_max_iter'])\n",
    "    \n",
    "    icp_threshold = params['voxel_size'] * params['icp_threshold_multiplier']\n",
    "\n",
    "    # Run ICP\n",
    "    result_icp = o3d.pipelines.registration.registration_icp(\n",
    "        source, target, icp_threshold, np.eye(4),\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "        config)\n",
    "    \n",
    "    # Update pcd based on the transformation matrix obtained from ICP\n",
    "    source.transform(result_icp.transformation)\n",
    "    return source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model\n",
    "# model, transform = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# def get_text_clip_embedding(text):\n",
    "#     text_inputs = clip.tokenize([text]).to(device)\n",
    "    \n",
    "#     # Get the text features\n",
    "#     with torch.no_grad():\n",
    "#         text_features = model.encode_text(text_inputs)\n",
    "        \n",
    "#     # Normalize the features\n",
    "#     text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "#     return text_features.cpu().squeeze().numpy()\n",
    "\n",
    "# ceiling_embed = get_text_clip_embedding(\"This is an image of a ceiling\")\n",
    "# wall_embed = get_text_clip_embedding(\"This is an image of a wall\")\n",
    "# floor_embed = get_text_clip_embedding(\"This is an image of floor\")\n",
    "# chair_embed = get_text_clip_embedding(\"This is an image of a chair\")\n",
    "# background_embed = get_text_clip_embedding(\"This is an image of a ceiling or wall or floor or pillar\")\n",
    "\n",
    "# del model\n",
    "# del transform\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obj Nodes Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pcd(pcd, params, run_dbscan=True):\n",
    "    pcd = pcd.voxel_down_sample(voxel_size=params['voxel_size'])\n",
    "\n",
    "    if run_dbscan:\n",
    "        pcd = fast_DBSCAN(pcd, eps=params['eps'], min_samples=params['min_samples'])\n",
    "\n",
    "    # points = np.asarray(pcd.points)\n",
    "    # colors = np.asarray(pcd.colors)  # Assuming your point cloud has colors\n",
    "\n",
    "    # # Filter out points below the ground plane and above the ceiling plane\n",
    "    # mask = (points[:, 1] >= 0) & (points[:, 1] <= 2)\n",
    "    # points_filtered = points[mask]\n",
    "    # colors_filtered = colors[mask]\n",
    "\n",
    "    # # Create a new point cloud with the filtered points\n",
    "    # pcd_filtered = o3d.geometry.PointCloud()\n",
    "    # pcd_filtered.points = o3d.utility.Vector3dVector(points_filtered)\n",
    "    # pcd_filtered.colors = o3d.utility.Vector3dVector(colors_filtered)\n",
    "    \n",
    "    return pcd\n",
    "\n",
    "def get_bounding_box(pcd):\n",
    "    try:\n",
    "        return pcd.get_oriented_bounding_box(robust=True)\n",
    "    except RuntimeError as e:\n",
    "        # print(f\"Met {e}, use axis aligned bounding box instead\")\n",
    "        return pcd.get_axis_aligned_bounding_box()\n",
    "\n",
    "def check_background(obj_data):\n",
    "    background_words = ['ceiling', 'wall', 'floor', 'pillar', 'door', 'basement', 'room', 'rooms', 'workshop', 'warehouse', 'building', \"apartment\", \"image\", \"city\", \"blue\", \"skylight\", \"hallway\", \n",
    "    \"bureau\", \"modern\", \"salon\", \"doorway\", \"house\", \"home\", \"carpet\", \"space\"]\n",
    "    background_phrase = [\"kitchen\", \"bedroom\", \"home office\", \"wood\", \"hardwood\", \"office\"]\n",
    "\n",
    "    # background_words = ['room', 'rooms']\n",
    "    # background_phrase = []\n",
    "    \n",
    "    obj_phrase = obj_data['phrase']\n",
    "    if obj_phrase in background_phrase:\n",
    "        return True\n",
    "\n",
    "    obj_words = obj_phrase.split()\n",
    "    for word in obj_words:\n",
    "        if word in background_words:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_nodes(node1, node2, params, run_icp=False):\n",
    "    # Merge source IDs: source_ids: [(img_id, obj_id), ...]\n",
    "    source_ids = node1['source_ids'] + node2['source_ids']\n",
    "    count = len(source_ids)\n",
    "\n",
    "    # Average the embeddings\n",
    "    avg_clip_embed = (np.array(node1['clip_embed']) * len(node1['source_ids']) +\n",
    "                      np.array(node2['clip_embed']) * len(node2['source_ids'])) / count\n",
    "\n",
    "    avg_dino_embed = (np.array(node1['dino_embed']) * len(node1['source_ids']) +\n",
    "                      np.array(node2['dino_embed']) * len(node2['source_ids'])) / count\n",
    "    \n",
    "    if run_icp:\n",
    "        node2['pcd'] = vanilla_icp(node2['pcd'], node1['pcd'], params)\n",
    "\n",
    "    # Combine point clouds\n",
    "    merged_pcd = node1['pcd']\n",
    "    merged_pcd.points.extend(node2['pcd'].points)\n",
    "\n",
    "    # make all points the same color (node1's color)\n",
    "    color = generate_pastel_color()\n",
    "    merged_pcd.colors = o3d.utility.Vector3dVector(np.tile(color, (len(merged_pcd.points), 1)))\n",
    "    merged_pcd = process_pcd(merged_pcd, params)\n",
    "\n",
    "    bbox = get_bounding_box(merged_pcd)\n",
    "\n",
    "    # Concatenate the points contributions from both nodes\n",
    "    points_contri = node1['points_contri'] + node2['points_contri']\n",
    "\n",
    "    return {\n",
    "        'source_ids': source_ids,\n",
    "        'clip_embed': avg_clip_embed,\n",
    "        'dino_embed': avg_dino_embed,\n",
    "        'pcd': merged_pcd,\n",
    "        'bbox': bbox,\n",
    "        'points_contri': points_contri\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_scene_nodes(img_data, img_id, params):\n",
    "    # Initialize an empty dictionary to store scene object nodes\n",
    "    scene_obj_nodes = {}\n",
    "\n",
    "    # Retrieve the initial image data using the provided ID\n",
    "    img_path = img_data['img_path']\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert image from BGR to RGB format\n",
    "\n",
    "    # Retrieve objects present in the image\n",
    "    objs = img_data['objs']\n",
    "    node_count = 1\n",
    "\n",
    "    for obj_id in objs.keys():\n",
    "        obj_data = objs[obj_id]\n",
    "\n",
    "        # Calculate similarities\n",
    "        check_background_flag = check_background(obj_data)\n",
    "        \n",
    "        if check_background_flag:\n",
    "            node_id = 0\n",
    "            continue # Skipping the background objects for now\n",
    "        else:\n",
    "            node_id = node_count\n",
    "            node_count += 1\n",
    "        \n",
    "        color = generate_pastel_color()\n",
    "        pcd = create_point_cloud(img_id, obj_data, params['cam_mat'], color=color, cam_height=params['cam_height'], depth_scale=params['depth_scale'])\n",
    "        pcd = process_pcd(pcd, params)\n",
    "\n",
    "        bbox = get_bounding_box(pcd)\n",
    "        if bbox.volume() < 1e-6 or len(pcd.points) < 10:\n",
    "            continue\n",
    "        \n",
    "        # Store the object data in the scene object nodes dictionary\n",
    "        scene_obj_nodes[node_id] = {'source_ids': [(img_id, obj_id)], \n",
    "                                    'clip_embed': objs[obj_id]['clip_embed'], \n",
    "                                    'dino_embed': objs[obj_id]['dino_embed'], \n",
    "                                    'pcd': pcd, \n",
    "                                    'bbox': bbox,\n",
    "                                    'points_contri': [len(pcd.points)]}  # Count of points in the point cloud\n",
    "\n",
    "    # print(\"Number of nodes in the scene: \", len(scene_obj_nodes))\n",
    "    return scene_obj_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlap_matrix_2set(scene_obj_nodes, det_nodes, params):\n",
    "    '''\n",
    "    Compute pairwise overlapping between two sets of objects in terms of point nearest neighbor.\n",
    "    scene_obj_nodes is the existing objects in the scene, det_nodes is the new objects to be added to the scene\n",
    "    '''\n",
    "\n",
    "    m = len(scene_obj_nodes)\n",
    "    n = len(det_nodes)\n",
    "    overlap_matrix = np.zeros((m, n))\n",
    "\n",
    "    # Convert the point clouds into numpy arrays and then into FAISS indices for efficient search\n",
    "    points_map = [np.asarray(obj['pcd'].points, dtype=np.float32) for obj in scene_obj_nodes.values()]\n",
    "    indices = [faiss.IndexFlatL2(arr.shape[1]) for arr in points_map] # m indices\n",
    "    \n",
    "    # Add the points from the numpy arrays to the corresponding FAISS indices\n",
    "    for index, arr in zip(indices, points_map):\n",
    "        index.add(arr)\n",
    "        \n",
    "    points_new = [np.asarray(obj['pcd'].points, dtype=np.float32) for obj in det_nodes.values()]\n",
    "    \n",
    "    # Assuming you can compute 3D IoU given the 'bbox' field in your dicts\n",
    "    bbox_map_np = np.array([obj['bbox'].get_box_points() for obj in scene_obj_nodes.values()])\n",
    "    bbox_map = torch.from_numpy(bbox_map_np).to(device)\n",
    "\n",
    "    bbox_new_np = np.array([obj['bbox'].get_box_points() for obj in det_nodes.values()])\n",
    "    bbox_new = torch.from_numpy(bbox_new_np).to(device)\n",
    "    \n",
    "    try:\n",
    "        # Assuming you have a function called compute_3d_iou_accurate_batch that takes PyTorch tensors\n",
    "        iou = compute_3d_iou_accuracte_batch(bbox_map, bbox_new)  # (m, n)\n",
    "    except ValueError:\n",
    "        # If you encounter the \"Plane vertices are not coplanar\" error, switch to axis-aligned bounding boxes\n",
    "        bbox_map = []\n",
    "        bbox_new = []\n",
    "        \n",
    "        for node in scene_obj_nodes.values():\n",
    "            bbox_map.append(np.asarray(\n",
    "                node['pcd'].get_axis_aligned_bounding_box().get_box_points()\n",
    "            ))\n",
    "\n",
    "        for node in det_nodes.values():\n",
    "            bbox_new.append(np.asarray(\n",
    "                node['pcd'].get_axis_aligned_bounding_box().get_box_points()\n",
    "            ))\n",
    "        \n",
    "        bbox_map = torch.tensor(np.stack(bbox_map))\n",
    "        bbox_new = torch.tensor(np.stack(bbox_new))\n",
    "        \n",
    "        # Assuming you have a function called compute_iou_batch that takes PyTorch tensors\n",
    "        iou = compute_iou_batch(bbox_map, bbox_new)  # (m, n)\n",
    "\n",
    "\n",
    "    # Compute the pairwise overlaps\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if iou[i, j] < 1e-6:\n",
    "                continue\n",
    "            \n",
    "            D, I = indices[i].search(points_new[j], 1)  # Search new object j in map object i\n",
    "\n",
    "            overlap = (D < params['voxel_size'] ** 2).sum()  # D is the squared distance\n",
    "\n",
    "            # Calculate the ratio of points within the threshold\n",
    "            overlap_matrix[i, j] = overlap / len(points_new[j])\n",
    "\n",
    "    return overlap_matrix\n",
    "\n",
    "def compute_spatial_similarity(scene_obj_nodes, det_nodes, params):\n",
    "    spatial_sim = compute_overlap_matrix_2set(scene_obj_nodes, det_nodes, params)\n",
    "    spatial_sim = torch.from_numpy(spatial_sim).T\n",
    "    spatial_sim = spatial_sim.to(device)\n",
    "\n",
    "    return spatial_sim\n",
    "\n",
    "def compute_visual_similarity(scene_obj_nodes, det_nodes, params):\n",
    "    '''\n",
    "    Compute the visual similarities between the detections and the objects.\n",
    "    \n",
    "    Args:\n",
    "        scene_obj_nodes: a dict of N objects in the scene\n",
    "        det_nodes: a dict of M detections\n",
    "    Returns:\n",
    "        A MxN tensor of visual similarities\n",
    "    '''\n",
    "    \n",
    "    # Extract clip embeddings from scene_obj_nodes and det_nodes dictionaries\n",
    "    embed_type = params['embed_type']\n",
    "    obj_fts = np.array([obj[embed_type] for obj in scene_obj_nodes.values()]) # (N, D)\n",
    "    det_fts = np.array([obj[embed_type] for obj in det_nodes.values()]) # (M, D)\n",
    "\n",
    "    obj_fts = torch.from_numpy(obj_fts).to(device)\n",
    "    det_fts = torch.from_numpy(det_fts).to(device)\n",
    "    \n",
    "    # Reshape tensors to match dimensions for cosine similarity\n",
    "    det_fts = det_fts.unsqueeze(-1)  # (M, D, 1)\n",
    "    obj_fts = obj_fts.T.unsqueeze(0)  # (1, D, N)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    visual_sim = F.cosine_similarity(det_fts, obj_fts, dim=1)  # (M, N)\n",
    "    \n",
    "    # Scale the visual similarity to be between 0 and 1\n",
    "    # scaled_visual_sim = (visual_sim + 1) / 2\n",
    "    \n",
    "    return visual_sim\n",
    "\n",
    "def compute_aggregate_similarity(scene_obj_nodes, det_nodes, params):\n",
    "    '''\n",
    "    Compute the aggregate similarity between the detections and the objects.\n",
    "    \n",
    "    Args:\n",
    "        scene_obj_nodes: a dict of N objects in the scene\n",
    "        det_nodes: a dict of M detections\n",
    "    Returns:\n",
    "        A MxN tensor of aggregate similarities\n",
    "    '''\n",
    "\n",
    "    spatial_sim = compute_spatial_similarity(scene_obj_nodes, det_nodes, params)\n",
    "    visual_sim  = compute_visual_similarity(scene_obj_nodes, det_nodes, params)\n",
    "    aggregate_sim = (1 + params['alpha']) * visual_sim + (1 - params['alpha']) * spatial_sim\n",
    "\n",
    "    # if value in row is less than threshold, set it to -inf\n",
    "    aggregate_sim[aggregate_sim < params['sim_threshold']] = -float('inf')\n",
    "\n",
    "    return aggregate_sim, spatial_sim, visual_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_scene_nodes(img_id, img_data, scene_obj_nodes, params):\n",
    "    det_nodes = init_scene_nodes(img_data, img_id, params)  # Assuming img_data should be used\n",
    "    \n",
    "    if len(det_nodes) == 0:\n",
    "        return scene_obj_nodes\n",
    "    \n",
    "    # Assuming you have a function named compute_aggregate_similarity to get aggregate similarity\n",
    "    aggregate_sim, _, _ = compute_aggregate_similarity(scene_obj_nodes, det_nodes, params)\n",
    "    \n",
    "    # Initialize a new dictionary to store updated scene_obj_nodes\n",
    "    updated_scene_obj_nodes = scene_obj_nodes.copy()\n",
    "    \n",
    "    # Find the maximum existing key in scene_obj_nodes\n",
    "    max_scene_key = max(scene_obj_nodes.keys(), default=0)\n",
    "    \n",
    "    # Iterate through all detected nodes to merge them into existing scene_obj_nodes\n",
    "    for i, det_key in enumerate(det_nodes.keys()):\n",
    "        # If not matched to any object in the scene, add it as a new object\n",
    "        if aggregate_sim[i].max() == float('-inf'):\n",
    "            new_key = max_scene_key + det_key  # Create a new unique key\n",
    "            updated_scene_obj_nodes[new_key] = det_nodes[det_key]\n",
    "        else:\n",
    "            # Merge with most similar existing object in the scene\n",
    "            j = aggregate_sim[i].argmax().item()\n",
    "            scene_key = list(scene_obj_nodes.keys())[j]\n",
    "            matched_det = det_nodes[det_key]\n",
    "            matched_obj = scene_obj_nodes[scene_key]\n",
    "            \n",
    "            # Merge the matched detection node into the matched scene node\n",
    "            updated_scene_obj_nodes[scene_key] = merge_nodes(matched_obj, matched_det, params)\n",
    "\n",
    "    # print(\"Number of nodes in the scene: \", len(updated_scene_obj_nodes))\n",
    "    \n",
    "    return updated_scene_obj_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_scene_nodes(scene_obj_nodes, params):\n",
    "    print(\"Before filtering:\", len(scene_obj_nodes))\n",
    "    \n",
    "    # Initialize a new dictionary to store the filtered scene_obj_nodes\n",
    "    filtered_scene_obj_nodes = {}\n",
    "    \n",
    "    for key, obj in scene_obj_nodes.items():\n",
    "        # Use len(obj['pcd'].points) to get the number of points and len(obj['source_ids']) to get the number of views\n",
    "        if len(obj['pcd'].points) >= params['obj_min_points'] and len(obj['source_ids']) >= params['obj_min_detections']:\n",
    "            filtered_scene_obj_nodes[key] = obj\n",
    "\n",
    "    print(\"After filtering:\", len(filtered_scene_obj_nodes))\n",
    "    \n",
    "    return filtered_scene_obj_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlap_matrix_nodes(params, scene_obj_nodes):\n",
    "    n = len(scene_obj_nodes)\n",
    "    overlap_matrix = np.zeros((n, n))\n",
    "    \n",
    "    point_arrays = [np.asarray(obj['pcd'].points, dtype=np.float32) for obj in scene_obj_nodes.values()]\n",
    "    indices = [faiss.IndexFlatL2(arr.shape[1]) for arr in point_arrays]\n",
    "    \n",
    "    for index, arr in zip(indices, point_arrays):\n",
    "        index.add(arr)\n",
    "    \n",
    "    for i, obj_i in enumerate(scene_obj_nodes.values()):\n",
    "        for j, obj_j in enumerate(scene_obj_nodes.values()):\n",
    "            if i != j:\n",
    "                box_i = obj_i['bbox']\n",
    "                box_j = obj_j['bbox']\n",
    "\n",
    "                if params['merge_overlap_method'] == 'iou':\n",
    "                    iou = compute_3d_iou(box_i, box_j)\n",
    "                    overlap_matrix[i, j] = iou\n",
    "\n",
    "                elif params['merge_overlap_method'] == 'max_overlap':\n",
    "                    iou = compute_3d_iou(box_i, box_j, use_iou=False)\n",
    "                    overlap_matrix[i, j] = iou\n",
    "                    \n",
    "                elif params['merge_overlap_method'] == 'nnratio':\n",
    "                    iou = compute_3d_iou(box_i, box_j)\n",
    "                    if iou == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    D, I = indices[j].search(point_arrays[i], 1)\n",
    "                    overlap = (D < params['voxel_size'] ** 2).sum()\n",
    "                    overlap_matrix[i, j] = overlap / len(point_arrays[i])\n",
    "    \n",
    "    return overlap_matrix\n",
    "\n",
    "def merge_overlap_nodes(params, scene_obj_nodes, overlap_matrix):\n",
    "    x, y = overlap_matrix.nonzero()\n",
    "    overlap_ratio = overlap_matrix[x, y]\n",
    "    \n",
    "    sort = np.argsort(overlap_ratio)[::-1]\n",
    "    x = x[sort]\n",
    "    y = y[sort]\n",
    "    overlap_ratio = overlap_ratio[sort]\n",
    "    \n",
    "    kept_keys = list(scene_obj_nodes.keys())\n",
    "    merged_keys = set()  # Keep track of keys that have been merged into others\n",
    "    \n",
    "    for i, j, ratio in zip(x, y, overlap_ratio):\n",
    "        key_i = kept_keys[i]\n",
    "        key_j = kept_keys[j]\n",
    "        \n",
    "        # Skip if these keys have been merged into others\n",
    "        if key_i in merged_keys or key_j in merged_keys:\n",
    "            continue\n",
    "\n",
    "        embed_type = params['embed_type']\n",
    "        visual_sim = F.cosine_similarity(\n",
    "            torch.tensor(scene_obj_nodes[key_i][embed_type]).to(device),\n",
    "            torch.tensor(scene_obj_nodes[key_j][embed_type]).to(device),\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        overall_sim = (1 + params['alpha']) * visual_sim + (1 - params['alpha']) * ratio\n",
    "        \n",
    "        if overall_sim > params['merge_overall_thresh']:\n",
    "            if key_j in scene_obj_nodes: # Check if key_j still exists\n",
    "                # scene_obj_nodes[key_j] = merge_nodes(scene_obj_nodes[key_j], scene_obj_nodes[key_i], params, run_icp=False)\n",
    "                scene_obj_nodes[key_j] = merge_nodes(scene_obj_nodes[key_j], scene_obj_nodes[key_i], params, run_icp=False)\n",
    "                del scene_obj_nodes[key_i]\n",
    "                merged_keys.add(key_i)  # Mark key_i as merged\n",
    "    \n",
    "    return scene_obj_nodes\n",
    "\n",
    "def merge_scene_nodes(scene_obj_nodes, params):\n",
    "    if params['merge_overall_thresh'] > 0:\n",
    "        print(\"Before merging:\", len(scene_obj_nodes))\n",
    "        \n",
    "        # Compute the overlap matrix\n",
    "        overlap_matrix = compute_overlap_matrix_nodes(params, scene_obj_nodes)\n",
    "        \n",
    "        # Merge overlapping nodes\n",
    "        scene_obj_nodes = merge_overlap_nodes(params, scene_obj_nodes, overlap_matrix)\n",
    "        \n",
    "        print(\"After merging:\", len(scene_obj_nodes))\n",
    "    \n",
    "    return scene_obj_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_nodes(scene_obj_nodes, params):\n",
    "    # Initialize a new dictionary to store the filtered scene_obj_nodes\n",
    "    filtered_scene_obj_nodes = {}\n",
    "    \n",
    "    for key, obj in scene_obj_nodes.items():\n",
    "        if len(obj['pcd'].points) > 10:\n",
    "            filtered_scene_obj_nodes[key] = obj\n",
    "\n",
    "    return filtered_scene_obj_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = \"/scratch/kumaraditya_gupta/checkpoints/\"\n",
    "\n",
    "dataset_dir = \"/scratch/kumaraditya_gupta/Datasets/arkitscenes/ChallengeDevelopmentSet/42445173\"\n",
    "version = \"v1\"\n",
    "\n",
    "# imgs_dir = os.path.join(dataset_dir, \"rgb\")\n",
    "# depth_dir = os.path.join(dataset_dir, \"depth\")\n",
    "# pose_dir =  os.path.join(dataset_dir, \"pose\")\n",
    "imgs_dir = os.path.join(dataset_dir, \"lowres_wide\")\n",
    "depth_dir = os.path.join(dataset_dir, \"lowres_depth\")\n",
    "pose_dir =  os.path.join(dataset_dir, \"poses\")\n",
    "\n",
    "output_dir = os.path.join(dataset_dir, f\"output_{version}\")\n",
    "img_dict_dir = os.path.join(output_dir, \"img_dict.pkl\")\n",
    "\n",
    "# Load from pickle file\n",
    "with open(img_dict_dir, 'rb') as file:\n",
    "    img_dict = pickle.load(file)\n",
    "\n",
    "scene_obj_node_path = os.path.join(output_dir, \"scene_obj_nodes.pkl\")\n",
    "with open(scene_obj_node_path, 'rb') as file:\n",
    "    scene_obj_nodes = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'init_img_id': '001', #initialize the scene with this image\n",
    "          'stride': 1,\n",
    "\n",
    "          'depth_scale': 1000, #depth scale for converting depth image to meters (std value: 1000.0, 655.35)\n",
    "          \n",
    "          'voxel_size': 0.025, #voxel size for downsampling point clouds (std value: 0.025, 0.05, 0.1)\n",
    "          'eps': 0.075, #eps for DBSCAN (std value: 0.075, 0.125, 0.25)\n",
    "          'min_samples': 10, #min_samples for DBSCAN (std value: 10)\n",
    "          'embed_type': 'dino_embed', #embedding type to use for visual similarity\n",
    "          \n",
    "          'sim_threshold': 0.95, #threshold for aggregate similarity while running update_scene_nodes (0.95, 1.2)\n",
    "          'alpha': 0, #weight for visual similarity while computing aggregate similarity\n",
    "\n",
    "          'merge_overlap_method': 'nnratio', #metric to use for merging overlapping nodes\n",
    "          'merge_overall_thresh' : 0.95, #threshold for overall similarity while merging nodes in scene (0.95, 1.2)\n",
    "\n",
    "          'obj_min_points': 50, #minimum number of points in a node while filtering scene nodes\n",
    "          'obj_min_detections': 5, #minimum number of detections in a node while filtering scene nodes\n",
    "\n",
    "          'icp_threshold_multiplier': 1.5, #threshold multiplier for ICP\n",
    "          'icp_max_iter': 2000, #maximum number of iterations for ICP\n",
    "\n",
    "          'cam_mat': get_kinect_cam_mat(), #camera matrix (get_sim_cam_mat_with_fov(900, 900, 90))\n",
    "          'cam_height': 1.5, #camera height\n",
    "\n",
    "          'img_size': (1536, 2048), #image size\n",
    "          'save_folder': '/scratch/kumaraditya_gupta/kinect_output_imgs/' #for vis, not being used right now\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only first 100 entries of img_dict\n",
    "# max_imgs = 50\n",
    "img_dict = {k: v for i, (k, v) in enumerate(img_dict.items()) if i % params[\"stride\"] == 0}\n",
    "# img_dict = {k: img_dict[k] for k in list(img_dict)[:max_imgs]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for every img, modify the img_path from /scratch/kumaradi.gupta/run_kinect_wheel_2/rgb/1.png to /scratch/kumaradi.gupta/Datasets/kinect_wheel_2/rgb/1.png\n",
    "# for img_id, img_data in tqdm(img_dict.items()):\n",
    "#     img_data['img_path'] = img_data['img_path'].replace('/scratch/kumaradi.gupta/run_iphone', '/scratch/kumaradi.gupta/Datasets/run_iphone')\n",
    "\n",
    "# print(img_dict['1']['img_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_obj_nodes = init_scene_nodes(img_dict[params['init_img_id']], \n",
    "                                    params['init_img_id'], \n",
    "                                    params)\n",
    "\n",
    "# visualize_and_capture(params['init_img_id'], scene_obj_nodes, params)\n",
    "print(\"Number of nodes in the scene: \", len(scene_obj_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for img_id, img_data in tqdm(img_dict.items()):\n",
    "    if len(img_data['objs']) == 0 or img_id == params['init_img_id']:\n",
    "        continue\n",
    "    \n",
    "    scene_obj_nodes = update_scene_nodes(img_id, img_data, scene_obj_nodes, params)\n",
    "    scene_obj_nodes = remove_empty_nodes(scene_obj_nodes, params)\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 25 == 0:\n",
    "        scene_obj_nodes = merge_scene_nodes(scene_obj_nodes, params)\n",
    "        scene_obj_nodes = remove_empty_nodes(scene_obj_nodes, params)\n",
    "        \n",
    "    # visualize_and_capture(img_id, scene_obj_nodes, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_obj_nodes = merge_scene_nodes(scene_obj_nodes, params)\n",
    "scene_obj_nodes = filter_scene_nodes(scene_obj_nodes, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_ids = []\n",
    "\n",
    "for node_id, node_data in scene_obj_nodes.items():\n",
    "    pcd_path = node_data['pcd']\n",
    "    pcd = o3d.io.read_point_cloud(pcd_path)\n",
    "\n",
    "    # Filter out points where y < 0\n",
    "    points = np.asarray(pcd.points)\n",
    "    colors = np.asarray(pcd.colors)  # Assuming your point cloud has colors\n",
    "    mask = points[:, 2] >= -1.5 #filter z\n",
    "    points_filtered = points[mask]\n",
    "    colors_filtered = colors[mask]\n",
    "    \n",
    "    if len(points_filtered) < 10:\n",
    "        # If fewer than 10 points remain, delete the node\n",
    "        remove_ids.append(node_id)\n",
    "    else:\n",
    "        # Update the point cloud and save it\n",
    "        pcd.points = o3d.utility.Vector3dVector(points_filtered)\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colors_filtered)\n",
    "\n",
    "        node_data['pcd'] = pcd  # Update the point cloud in your data structure\n",
    "        node_data['bbox'] = get_bounding_box(pcd)\n",
    "\n",
    "for node_id in remove_ids:\n",
    "    del scene_obj_nodes[node_id]\n",
    "\n",
    "print(\"Number of objs in the scene: \", len(scene_obj_nodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pcd_dir = os.path.join(output_dir, \"pcds\")\n",
    "if not os.path.exists(save_pcd_dir):\n",
    "    os.makedirs(save_pcd_dir)\n",
    "\n",
    "print(\"Number of objs in the scene: \", len(scene_obj_nodes))\n",
    "for node_id, node_data in scene_obj_nodes.items():\n",
    "    pcd_path = os.path.join(save_pcd_dir, f\"{node_id}.pcd\")\n",
    "    o3d.io.write_point_cloud(pcd_path, node_data['pcd'])\n",
    "    node_data['pcd'] = pcd_path  # Replace the Open3D object with the file path\n",
    "    node_data['bbox'] = np.array(node_data['bbox'].get_box_points())  # Convert the Open3D bounding box to a NumPy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the modified dictionary using pickle\n",
    "with open(os.path.join(output_dir, 'scene_obj_nodes.pkl'), 'wb') as f:\n",
    "    pickle.dump(scene_obj_nodes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top-k CLIP Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "with open(os.path.join(output_dir, 'scene_obj_nodes.pkl'), 'rb') as f:\n",
    "    scene_obj_nodes = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image, bounding_box):\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    x, y, w, h = bounding_box\n",
    "    xmin = int((x - w/2).item() * width)\n",
    "    ymin = int((y - h/2).item() * height)\n",
    "    xmax = int((x + w/2).item() * width)\n",
    "    ymax = int((y + h/2).item() * height)\n",
    "    cropped_image = image[ymin:ymax, xmin:xmax]\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "def multi_scale_crop_image(image, bounding_box, kexp=0.1, levels=[0, 1, 2]):\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    x, y, w, h = bounding_box\n",
    "    xmin = int((x - w/2).item() * width)\n",
    "    ymin = int((y - h/2).item() * height)\n",
    "    xmax = int((x + w/2).item() * width)\n",
    "    ymax = int((y + h/2).item() * height)\n",
    "\n",
    "    # Initialize list to hold cropped images\n",
    "    cropped_images = []\n",
    "\n",
    "    # Iterate over each level\n",
    "    for l in levels:\n",
    "        # Calculate new bounding box coordinates\n",
    "        xl1 = max(0, xmin - (xmax - xmin) * kexp * l)\n",
    "        yl1 = max(0, ymin - (ymax - ymin) * kexp * l)\n",
    "        xl2 = min(xmax + (xmax - xmin) * kexp * l, width - 1)\n",
    "        yl2 = min(ymax + (ymax - ymin) * kexp * l, height - 1)\n",
    "\n",
    "        # Crop and append the image\n",
    "        cropped_images.append(image[int(yl1):int(yl2), int(xl1):int(xl2)])\n",
    "\n",
    "    return cropped_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\n",
    "        \"ViT-H-14\", \"laion2b_s32b_b79k\"\n",
    "    )\n",
    "clip_model = clip_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node_id in tqdm(scene_obj_nodes.keys()):\n",
    "    # For k highest points_contri, find the corresponding source_ids\n",
    "    k = 3\n",
    "    points_contri = scene_obj_nodes[node_id]['points_contri']\n",
    "    source_ids = scene_obj_nodes[node_id]['source_ids']\n",
    "\n",
    "    sort = np.argsort(points_contri)[::-1]\n",
    "    sorted_source_ids = np.array(source_ids)[sort][:k]\n",
    "\n",
    "    # List to hold all crops from all top-k images\n",
    "    all_crops = []\n",
    "\n",
    "    # Process each source_id, avoid reading and processing the same image multiple times\n",
    "    processed_images = {}\n",
    "    for source_id in sorted_source_ids:\n",
    "        img_id, obj_id = source_id[0], source_id[1]\n",
    "\n",
    "        # Read and preprocess the image only once per unique img_id\n",
    "        if img_id not in processed_images:\n",
    "            img_path = img_dict[img_id]['img_path']\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert image from BGR to RGB format\n",
    "            processed_images[img_id] = img\n",
    "        else:\n",
    "            img = processed_images[img_id]\n",
    "\n",
    "        obj_bbox = img_dict[img_id][\"objs\"][int(obj_id)][\"bbox\"]\n",
    "        cropped_imgs = multi_scale_crop_image(img, obj_bbox, levels=[0, 1, 2])\n",
    "        all_crops.extend(cropped_imgs)\n",
    "\n",
    "    # Convert all crops to CLIP-preprocessed tensors\n",
    "    all_crops_tensors = [clip_preprocess(Image.fromarray(crop)).unsqueeze(0) for crop in all_crops]\n",
    "    all_crops_tensors = torch.cat(all_crops_tensors).to(device)\n",
    "\n",
    "    # Get CLIP embeddings in one shot\n",
    "    with torch.no_grad():\n",
    "        all_clip_features = clip_model.encode_image(all_crops_tensors)\n",
    "        all_clip_features /= all_clip_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    avg_clip_features = all_clip_features.mean(dim=0)\n",
    "    avg_clip_features /= avg_clip_features.norm()\n",
    "\n",
    "    scene_obj_nodes[node_id]['multi_clip_embed'] = avg_clip_features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the modified dictionary using pickle\n",
    "with open(os.path.join(output_dir, 'scene_obj_nodes.pkl'), 'wb') as f:\n",
    "    pickle.dump(scene_obj_nodes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dir = \"/scratch/kumaraditya_gupta/Datasets/arkitscenes/ChallengeDevelopmentSet/42445173\"\n",
    "# version = \"v1\"\n",
    "\n",
    "# imgs_dir = os.path.join(dataset_dir, \"color\")\n",
    "# depth_dir = os.path.join(dataset_dir, \"depth\")\n",
    "# pose_dir =  os.path.join(dataset_dir, \"pose\")\n",
    "\n",
    "img_ids = list(img_dict.keys())\n",
    "img_id = img_ids[0]\n",
    "\n",
    "depth_data = get_depth(img_id, depth_scale=1000)\n",
    "\n",
    "max_depth_value = depth_data.max()\n",
    "print(f\"Max depth value: {max_depth_value}\")\n",
    "\n",
    "# only keep the depth values that are within the valid range\n",
    "# depth_data[depth_data < 0] = 0\n",
    "# depth_data[depth_data > 3] = 0\n",
    "\n",
    "# Visualize the depth data as a heatmap\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(depth_data, cmap='viridis')  # You can choose different colormaps like 'gray', 'plasma', etc.\n",
    "plt.colorbar(label='Depth Value')\n",
    "plt.title('Depth Visualization')\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Height')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_id_1 = 82\n",
    "node_id_2 = 164\n",
    "\n",
    "sources = scene_obj_nodes[node_id_1]['source_ids']\n",
    "points_contri = scene_obj_nodes[node_id_1]['points_contri']\n",
    "\n",
    "tags_to_remove = [\"bathrooom\", \"exhaust\", \"air\", \"bath\"]\n",
    "\n",
    "# get the sources for top 5 points_contri\n",
    "points_contri = np.array(points_contri)\n",
    "top_5_idx = np.argsort(points_contri)[::-1][:5]\n",
    "sources = [sources[i] for i in top_5_idx]\n",
    "\n",
    "for source in sources:\n",
    "    img_id, obj_id = source\n",
    "    print(img_dict[img_id]['objs'][obj_id]['phrase'])\n",
    "\n",
    "# print(f\"Node 1 sources: {scene_obj_nodes[node_id_1]['source_ids']}\")\n",
    "# pcd1_img_id, pcd1_obj_id = scene_obj_nodes[node_id_1]['source_ids'][0]\n",
    "# print(img_dict[pcd1_img_id]['objs'][pcd1_obj_id]['phrase'])\n",
    "\n",
    "# print(f\"Node 2 sources: {scene_obj_nodes[node_id_2]['source_ids']}\")\n",
    "# pcd2_img_id, pcd1_obj_id = scene_obj_nodes[node_id_2]['source_ids'][1]\n",
    "# print(img_dict[pcd2_img_id]['objs'][pcd1_obj_id]['phrase'])\n",
    "\n",
    "\n",
    "# # make a new scene_obj_nodes with only the two nodes\n",
    "# node1 = {node_id_1: scene_obj_nodes[node_id_1]}\n",
    "# node2 = {node_id_2: scene_obj_nodes[node_id_2]}\n",
    "# aggregate_sim, spatial_sim, visual_sim = compute_aggregate_similarity(node1, node2, params)\n",
    "\n",
    "# print(f\"Aggregate similarity: {aggregate_sim}\")\n",
    "# print(f\"Spatial similarity: {spatial_sim}\")\n",
    "# print(f\"Visual similarity: {visual_sim}\")\n",
    "\n",
    "# node1_bbox = node1[node_id_1]['bbox']\n",
    "# node2_bbox = node2[node_id_2]['bbox']\n",
    "# max_overlap = compute_3d_iou(node1_bbox, node2_bbox, use_iou=False)\n",
    "# print(f\"3D Overlap: {max_overlap}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"/scratch/kumaraditya_gupta/Datasets/mp3d_2\"\n",
    "\n",
    "# open npy file K3x3.npy\n",
    "K = np.load(os.path.join(test_path, \"K3x3.npy\"))\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_multi_scale_bounding_boxes(image, bounding_box, kexp=0.1, levels=[0, 1, 2]):\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    x, y, w, h = bounding_box\n",
    "    xmin = int((x - w/2).item() * width)\n",
    "    ymin = int((y - h/2).item() * height)\n",
    "    xmax = int((x + w/2).item() * width)\n",
    "    ymax = int((y + h/2).item() * height)\n",
    "\n",
    "    # Colors for different levels of bounding boxes\n",
    "    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]  # Blue, Green, Red\n",
    "\n",
    "    # Iterate over each level\n",
    "    for idx, l in enumerate(levels):\n",
    "        # Calculate new bounding box coordinates\n",
    "        xl1 = max(0, xmin - (xmax - xmin) * kexp * l)\n",
    "        yl1 = max(0, ymin - (ymax - ymin) * kexp * l)\n",
    "        xl2 = min(xmax + (xmax - xmin) * kexp * l, width - 1)\n",
    "        yl2 = min(ymax + (ymax - ymin) * kexp * l, height - 1)\n",
    "\n",
    "        # Draw rectangle on the image\n",
    "        cv2.rectangle(image, (int(xl1), int(yl1)), (int(xl2), int(yl2)), colors[idx], 2)\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_id = 100\n",
    "\n",
    "# For k highest points_contri, find the corresponding source_ids\n",
    "k = 3\n",
    "points_contri = scene_obj_nodes[node_id]['points_contri']\n",
    "source_ids = scene_obj_nodes[node_id]['source_ids']\n",
    "\n",
    "sort = np.argsort(points_contri)[::-1]\n",
    "sorted_points_contri = np.array(points_contri)[sort]\n",
    "sorted_source_ids = np.array(source_ids)[sort]\n",
    "\n",
    "# print(sorted_points_contri)\n",
    "# print(sorted_source_ids)\n",
    "\n",
    "top_k_points_contri = sorted_points_contri[:k]\n",
    "top_k_source_ids = sorted_source_ids[:k]\n",
    "\n",
    "for source_id in top_k_source_ids:\n",
    "    img_id, obj_id = source_id[0], source_id[1]\n",
    "    img_path = img_dict[img_id]['img_path']\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert image from BGR to RGB format\n",
    "    \n",
    "    obj_bbox = img_dict[img_id][\"objs\"][int(obj_id)][\"bbox\"]\n",
    "    # cropped_img = crop_image(img, obj_bbox)\n",
    "\n",
    "    # cropped_imgs = multi_scale_crop_image(img, obj_bbox, levels=[0, 1, 2])\n",
    "    bbox_img = draw_multi_scale_bounding_boxes(img, obj_bbox, kexp=0.1)\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(bbox_img)\n",
    "    plt.show()\n",
    "    # for cropped_img in cropped_imgs:\n",
    "    #     plt.figure(figsize=(4, 4))\n",
    "    #     plt.imshow(cropped_img)\n",
    "    #     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to draw the segmentation mask on the image\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def generate_pastel_color():\n",
    "    return (\n",
    "        int(random.uniform(0.5, 1.0) * 255), \n",
    "        int(random.uniform(0.5, 1.0) * 255), \n",
    "        int(random.uniform(0.5, 1.0) * 255)\n",
    "    )\n",
    "\n",
    "# Load the image\n",
    "img_id = \"img_id\"\n",
    "img_path = img_dict[img_id]['img_path']\n",
    "img = Image.open(img_path)  # Load with PIL to avoid needing to convert color spaces\n",
    "\n",
    "H, W = img.size\n",
    "print(f\"Image size: {W}x{H}\")\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "img_tensor = transform(img)\n",
    "img_tensor = (img_tensor * 255).byte()\n",
    "\n",
    "masks = []\n",
    "mask_colors = []\n",
    "\n",
    "objs = img_dict[img_id]['objs']\n",
    "\n",
    "for obj_index, obj_data in objs.items():\n",
    "    mask = obj_data['mask']\n",
    "\n",
    "    phrase = obj_data['phrase']\n",
    "    phrase = phrase.split()[0] #Use only the first phrase\n",
    "\n",
    "    # If the mask isn't a numpy array, convert it to one\n",
    "    if not isinstance(mask, np.ndarray):\n",
    "        mask = mask.numpy()\n",
    "\n",
    "    # Add the mask to the list of masks\n",
    "    masks.append(mask)\n",
    "    #Cheeck the color for the mask and add it to the list\n",
    "    color = generate_pastel_color()\n",
    "    mask_colors.append(color)\n",
    "\n",
    "# Convert the masks to a boolean tensor\n",
    "masks_tensor = torch.tensor(masks, dtype=torch.bool)\n",
    "result = draw_segmentation_masks(img_tensor, masks_tensor, colors=mask_colors)\n",
    "result_img = Image.fromarray(result.permute(1, 2, 0).byte().cpu().numpy())\n",
    "\n",
    "# Display the image with all masks\n",
    "plt.imshow(result_img)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "o3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
